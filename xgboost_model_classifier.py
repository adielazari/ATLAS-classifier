# -*- coding: utf-8 -*-
"""XGBoost model classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OZ0QfG0QWiwFsk3C8I4cx0EuCgeloF5e
"""

import pandas as pd
import numpy as np
from xgboost import plot_tree
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from google.colab import drive
drive.mount('/content/drive/')

"""Reading data from 7 or 10 data simulation files (data7 or data respectively)"""

#data
data = pd.read_csv('drive/Shared drives/ATLAS project/data/data7.csv')

# add features
data['delta_z'] = np.abs(data['Track_z0']-data['Prime_vx_z'])
data["sin_teta"]= np.sin(data["Track_theta"])
data["Track_eta"]= np.cos(data["Track_eta"])
data = data.drop(columns=['Track_theta','Unnamed: 0'])

# target
target = pd.read_csv('drive/Shared drives/ATLAS project/data/target7.csv')

# Convert the type on target to boolean 
class_as_list =[]
for i in target["class"]:
  # class true is a track that related to primery vertex that truth with true track (1) and vertex (2) of simulation
  if i == 1 or i == 2:
    class_as_list.append(True)
  else:
    class_as_list.append(False)
y = pd.DataFrame({'class':class_as_list})
print(data.head())
print(data.shape)
print(y.shape)

"""Cutting data on Omega variable that define the distanse from zero point of primery vertex.
We will see when we don't cut, a accuracy is high, but the error of 
FPR (error of false we said are true) is also high.
But if we cut on +- 2 truly a accuracy is less high, but a error of FPR smaller than 2% so we will cut thus.
"""

#cutting of absulute 2 on Track omega feature 
msk = data.index[np.abs(data['Track_omega'])<2]
data = data.iloc[msk]
y = y.iloc[msk]

data

y

print(data.shape[1])
print(y.shape[1])

"""The data is imbalance. 
If don't cut by Omega variable the tendency in favor of false, because all detector space state on 300 mm but the primery vertex exist in around 4 mm, but if cutting a Omega on +-2 the tendency in favor of true.
"""

def data_clas_info(X,c_y):
  print("-------------DATA INFO------------")
  count_t = 0
  for i in c_y["class"]:
    if i is True:
      count_t+=1
  count_f = 0                                                                                                                            
  for i in c_y["class"]:
    if i is False:
      count_f+=1
  print('Num of Data: {}'.format(len(c_y)))
  
  print('true rate: {}'.format(count_t/len(c_y)))
  print('false rate: {}'.format(count_f/len(c_y)))

  return [count_t,count_f,count_t/len(c_y),count_f/len(c_y)]

count_t,count_f, ratio_t ,ratio_f= data_clas_info(data.copy(),y.copy())

"""There is 9 consecutive variables and target variable binary that selected by a physical specialist"""

print("data:")
print(data.dtypes)
print("\n")
y.dtypes

def get_cut_data_by_treshold(data_, y_,pred_,is_bef_train,class_name,treshold):
  data_y = data_ 
  data_y[class_name] = y_[class_name]
  c_pred = None    
  if(not is_bef_train):        
    data_y['pred'] = pred_["pred"]
    c_pred = data_y.loc[np.logical_and(np.abs(data_y['Track_omega'])>treshold,np.abs(data_y['Track_omega'])<=treshold+0.1),['pred']]
  c_X = data_y.loc[np.logical_and(np.abs(data_y['Track_omega'])>treshold,np.abs(data_y['Track_omega'])<=treshold+0.1),'EventNumber':'sin_teta']
  c_y = data_y.loc[np.logical_and(np.abs(data_y['Track_omega'])>treshold,np.abs(data_y['Track_omega'])<=treshold+0.1),[class_name]]
  return c_X,c_y ,c_pred

def create_graph(x_axis,y_axis,tresholds,x_label,y_label,title):
  lists = [x_axis,y_axis,tresholds]
  df_to_graph = pd.DataFrame.from_records(lists).T
  df_to_graph.columns = [x_label, y_label, 'Thresholds']
  ax = df_to_graph.plot(x='Thresholds',  title=title)
  ax.set_xlabel("Decision Threshold")
  ax.set_ylabel("Score")

"""The data are unbalanced and therefore we will see the relationship between the True classification and the False classification at the significant points of the study and the cross-sectional points of the omega variable"""

ratios_f = []
ratios_t = []
tresholds=[]
for i in np.linspace(0,1.9,num=20):
  treshold = round(i,1)
  print(treshold)
  tresholds.append(treshold)
  c_X,c_y,_ = get_cut_data_by_treshold(data.copy(),y.copy(),None,True,'class',treshold)
  c_t, c_f,ratio_t,ratio_f = data_clas_info(c_X,c_y)
  ratios_f.append(ratio_f)
  ratios_t.append(ratio_t)
create_graph(ratios_t,ratios_f,tresholds,'True', 'False','True/False Tradeoff')

#calc the preformence of the model
from sklearn.metrics import confusion_matrix, accuracy_score,roc_auc_score
def get_score(y_train,pred_train,y_test,pred_test):
  print("#############scores on train set################") 
  confm = confusion_matrix(y_train, pred_train)
  print("confusion matrix: ", confm)
  tn, fp, fn, tp = confm.ravel()
  print("tn:",tn, "fp:", fp, "fn:" ,fn,"tp:" ,tp)

  # Error rate : 
  err_rate = (fp + fn) / (tp + tn + fn + fp)
  print("Error rate: ", err_rate)
  # Accuracy : 
  acc_ = (tp + tn) / (tp + tn + fn + fp)
  print("Accuracy: ", acc_)
  # Sensitivity (tpr): 
  tpr = tp / (tp + fn)
  print("Sensitivity(tpr): ", tpr)
  # Specificity 
  sens = tn / (tn + fp)
  print("Specificity: ", sens)
  # False positive rate (FPR)
  fpr = fp / (tn + fp)
  print("False positive rate: ", fpr)
  #Roc auc score
  roc_auc = roc_auc_score(y_train, pred_train)
  print("roc_auc score: ", roc_auc)
  print("")
  print("#############scores on test set################")
  confm = confusion_matrix(y_test, pred_test)
  print("confusion matrix: ", confm)
  tn, fp, fn, tp = confm.ravel()
  print("tn:",tn, "fp:", fp, "fn:" ,fn,"tp:" ,tp)
  # Error rate : 
  err_rate = (fp + fn) / (tp + tn + fn + fp)
  print("Error rate: ", err_rate)
  # Accuracy : 
  acc = (tp + tn) / (tp + tn + fn + fp)
  print("Accuracy: ", acc)
  # Sensitivity (tpr): 
  tpr = tp / (tp + fn)
  print("Sensitivity(tpr): ", tpr)
  # Specificity 
  sens = tn / (tn + fp)
  print("Specificity: ", sens)
  # False positive rate (FPR)
  fpr = fp / (tn + fp)
  print("False positive rate: ", fpr)
  #Roc auc score
  roc_auc = roc_auc_score(y_test, pred_test)
  print("roc_auc score: ", roc_auc)

"""# Training and evaluations the model"""

from sklearn.model_selection import train_test_split

X=data
X_train, X_test, y_train, y_test = train_test_split(X, y["class"], test_size=0.33, random_state=42)

def get_model(model,name_model):
  model.fit(X_train, y_train,eval_set = [(X_train, y_train), (X_test, y_test)], eval_metric = ["auc","error"])
  return model

def predict(model):
  pred_train = model.predict(X_train)
  pred_test = model.predict(X_test)
  return [pred_train,pred_test]

def graph_metrics(model,name_model):
  # retrieve performance metrics
  results = model.evals_result()
  epochs = len(results['validation_0']['error'])
  x_axis = range(0, epochs)
  fig, ax = plt.subplots(1, 2, figsize=(15,5))
  # plot auc
  ax[0].plot(x_axis, results['validation_0']['auc'], label='Train')
  ax[0].plot(x_axis, results['validation_1']['auc'], label='Test')
  ax[0].legend()
  ax[0].set_title(name_model +' AUC-ROC')
  ax[0].set_ylabel('AUC-ROC')
  ax[0].set_xlabel('N estimators')
  # plot classification error
  ax[1].plot(x_axis, results['validation_0']['error'], label='Train')
  ax[1].plot(x_axis, results['validation_1']['error'], label='Test')
  ax[1].legend()
  ax[1].set_title(name_model + ' Classification Error')
  ax[1].set_ylabel('Classification Error')
  ax[1].set_xlabel('N estimators')
  plt.show()
  plt.tight_layout()

#start process
from xgboost import XGBClassifier
print("-------------XGBClassifier------------")
modelXGBOOST=XGBClassifier(n_estimator=400, random_state=1,learning_rate=0.01,tree_method = 'exact',min_child_weight=1, scale_pos_weight=count_f/count_t)
modelXGB = get_model(modelXGBOOST,"XGBoost")
pred_train,pred_test = predict(modelXGB)
get_score(y_train,pred_train,y_test,pred_test)
print("\r\r\r\r")

print('Train Accuracy: ', accuracy_score(y_train, pred_train))
print('Test Accuraccy: ', accuracy_score(y_test, pred_test))
print('Classification Report:')
print(classification_report(y_test,pred_test))
graph_metrics(modelXGB,"XGBoost")

# Create the feature importances plot on tree to show to Physical specialist
fig, ax = plt.subplots(figsize=(20,20))
plot_tree(modelXGB,ax = ax, rankdir='LR')
plt.show()

import torch
# save the model to disk
def save_model(filename,model):
  path = F"/content/drive/Shared drives/ATLAS project/models/{filename}" 
  torch.save(model, path)

save_model('model__with_all_features_and_7_xAOD_and_cut_on_2.sav',modelXGB)

def load_model(filename):
  # load the model from disk
  path = F"/content/drive/Shared drives/ATLAS project/models/{filename}"
  return torch.load(path)

modelXGB = load_model('model_XGBoost_with_all_features_and_7_xAOD_and_cut_on_2.sav')
pred_train,pred_test = predict(modelXGB)
get_score(y_train,pred_train,y_test,pred_test)

"""# The research question
After performing the model we want to use a correlation matrix to identify the state of our data after training. Having seen above that the indices provide in terms of accuracy, tpr and ROC AUC we want to understand if when we cut the data in the omega variable we will reduce the number of False positive (fp) * which will aim for zero. The number of fp in relation to the amount of false we saw in the graph above is significant to us, so although we are interested in the FPR index and the amount of fp in relation to the omega cross section.
* False positive (fp) - The amount of records we have classified as true even though their classification is false.
"""

# def metric_treshold_omega_var(X_test,y_test,pred_test):
_fp = []
_fn = []
_fpr = []
_fnr = [] 
_err = []
_acc = []
tresholds=[]
data_test = X_test.copy()
data_test['y'] = y_test.copy() 
data_test['pred'] = pred_test.copy()
x_te = data_test.loc[:,'EventNumber':'sin_teta']
y_te = data_test[['y']]
pred_te = data_test[['pred']]  
for i in np.linspace(0,1.9,num=20):
  treshold = round(i,1)
  print(treshold)
  tresholds.append(treshold)
  c_X,c_y,c_pred = get_cut_data_by_treshold(x_te.copy(),y_te.copy(),pred_te.copy(),False,'y',treshold)  
  confm = confusion_matrix(c_y, c_pred)
  tn, fp, fn, tp = confm.ravel()
  _fp.append(fp)
  print("fp:",fp)
  _fpr.append(fp/(fp+tn))
  print("fp/fp+tn (fpr):",fp/(fp+tn))
  _fn.append(fn) 
  print("fn:",fn)
  _fnr.append(fn/(fn+tp))
  print("fn/fn+tp (fnr):",fn/(fn+tp))
  _err.append((fp + fn) / (tp + tn + fn + fp))
  print("fp+fn/tp+tn+fn+fp (err):",(fp + fn) / (tp + tn + fn + fp))
  _acc.append((tp + tn) / (tp + tn + fn + fp))
  print("tp+tn/tp+tn+fn+fp (acc):",(tp + tn) / (tp + tn + fn + fp))
create_graph(_acc,_err,tresholds,'Acc', 'Err','Acc/Err Tradeoff')
create_graph(_fpr,_fnr,tresholds,'Fpr', 'Fnr','Fpr/Fnr Tradeoff')
create_graph(_fp,_fn,tresholds,'Acc', 'Err','Fp/Fn Tradeoff')

"""# feature importance
We received the model variables from physical experts but did not have an understanding and knowledge of what the significant variables were to create the data for the model. Therefore after using the physical knowledge we want to examine through technological tools what the significant variables are.
"""

from sklearn.feature_selection import SelectFromModel
from xgboost import plot_importance
import matplotlib.pyplot as plt
from numpy import sort
from sklearn.metrics import accuracy_score

# define custom class to fix bug in xgboost 1.0.2
class MyXGBClassifier(XGBClassifier):
	@property
	def coef_(self):
		return None
		
def features_selection(model,newModel):
	# feature importance
	print(model.feature_importances_)
	# Fit model using each importance as a threshold
	plot_importance(model)
	plt.show()
	thresholds = sort(model.feature_importances_)
	for thresh in thresholds:
		# select features using threshold
		selection = SelectFromModel(model, threshold=thresh, prefit=True)
		select_X_train = selection.transform(X_train)
  	# train model
		selection_model=newModel
		selection_model.fit(select_X_train, y_train)
  	# eval model
		select_X_test = selection.transform(X_test)
  	# y_pred = selection_model.predict(select_X_test)
  	# predictions = [round(value) for value in y_pred]
		predictions = selection_model.predict(select_X_test)
		accuracy = accuracy_score(y_test, predictions)
		print("Thresh=%.3f, n=%d, Accuracy: %.2f%%" % (thresh, select_X_train.shape[1], accuracy*100.0))

from xgboost import XGBClassifier
print("-------------NewXGBClassifier------------")
newModelXGBOOST=XGBClassifier(n_estimator=400, random_state=1,learning_rate=0.01,min_child_weight=1, scale_pos_weight=count_f/count_t)
features_selection(modelXGB, newModelXGBOOST)
print("\r\r\r\r")